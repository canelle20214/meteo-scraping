{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d2e16e",
   "metadata": {},
   "source": [
    "# Groupe 2 : récolte d'informations météorologiques en Ile de France\n",
    "Sites utilisés : https://www.linternaute.com/ville/ile-de-france/region-11/villes et https://www.meteo-villes.com/previsions-meteo-{ville}-{codepostale}.\n",
    "Framework : Scrapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28d05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99d2aa",
   "metadata": {},
   "source": [
    "## 1. Création des classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefe2fc",
   "metadata": {},
   "source": [
    "Pour commencer, nous créons les classes correspondant aux villes et à la météo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e70453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VilleItem(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    cp = scrapy.Field()\n",
    "\n",
    "class MeteoItem(scrapy.Item):\n",
    "    low = scrapy.Field()\n",
    "    hight = scrapy.Field()\n",
    "    wind = scrapy.Field()\n",
    "    rain = scrapy.Field()\n",
    "    date = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f82e7",
   "metadata": {},
   "source": [
    "Ensuite, une classe représentant la base de données Sqlite avec les méthodes basiques essentielles à son utilisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf4dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "\n",
    "class DataBase():\n",
    "    def __init__(self, name_database=\"database\"):\n",
    "        self.name = name_database\n",
    "        self.url = f\"sqlite:///{name_database}.db\"\n",
    "        self.engine = db.create_engine(self.url)\n",
    "        self.connection = self.engine.connect()\n",
    "        self.metadata = db.MetaData()\n",
    "        self.table = self.engine.get_table_names() \n",
    "\n",
    "    def create_table(self, name_table, **kwargs):\n",
    "        colums = [db.Column(k, v, primary_key = True) if 'id_' in k else db.Column(k, v) for k,v in kwargs.items()]\n",
    "        db.Table(name_table, self.metadata, *colums)\n",
    "        self.metadata.create_all(self.engine)\n",
    "        print(f\"Table : '{name_table}' are created succesfully\")\n",
    "\n",
    "    def read_table(self, name_table, return_keys=False):\n",
    "        table = db.Table(name_table, self.metadata, autoload=True, autoload_with=self.engine)\n",
    "        if return_keys:table.columns.keys() \n",
    "        else : return table\n",
    "\n",
    "    def add_row(self, name_table, **kwarrgs):\n",
    "        name_table = self.read_table(name_table)\n",
    "        stmt = (\n",
    "            db.insert(name_table).\n",
    "            values(kwarrgs)\n",
    "        )\n",
    "        self.connection.execute(stmt)\n",
    "        print(f'Row id added')\n",
    "\n",
    "\n",
    "    def delete_row_by_id(self, table, id_):\n",
    "        name_table = self.read_table(name_table) \n",
    "        stmt = (\n",
    "            db.delete(name_table).\n",
    "            where(students.c.id_ == id_)\n",
    "            )\n",
    "        self.connection.execute(stmt)\n",
    "        print(f'Row id {id_} deleted')\n",
    "\n",
    "    def select_table(self, name_table):\n",
    "        name_table = self.read_table(name_table)       \n",
    "        stm = db.select([name_table])\n",
    "        return self.connection.execute(stm).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d7c65",
   "metadata": {},
   "source": [
    "##  2. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698ed73",
   "metadata": {},
   "source": [
    "Dans un premier temps, `VilleSpider` récupère la liste des villes d'Ile de France sur le site l'`internaute.com`. Le nom de la ville ainsi que son code postal seront utils à la recherche de la météo journalière plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aebd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Request\n",
    "import unidecode\n",
    "\n",
    "class VilleSpider(scrapy.Spider):\n",
    "    name = 'ville'\n",
    "    allowed_domains = ['www.linternaute.com']\n",
    "    start_urls = ['https://www.linternaute.com/ville/ile-de-france/region-11/villes']\n",
    "\n",
    "    try:\n",
    "      base = DataBase('region')\n",
    "      base.create_table('ville', name=db.String, cp=db.String)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "          yield Request(url=url, callback=self.parse_ville)\n",
    "            \n",
    "    def parse_ville(self, response):\n",
    "        liste_villes = response.css('.list--bullet')[1].css('li')\n",
    "\n",
    "        for ville in liste_villes:\n",
    "            item = VilleItem() \n",
    "            \n",
    "            #nom de la ville\n",
    "            try: \n",
    "              item['name'] = unidecode.unidecode(\"-\".join(ville.css('a::text').extract()[0].split(' ')[:-1]).lower())\n",
    "            except:\n",
    "              item['name'] = 'None'\n",
    "            \n",
    "            #code postal de la ville\n",
    "            try: \n",
    "              item['cp'] = ville.css('a::text').extract()[0].split(' ')[-1].replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            except:\n",
    "              item['cp'] = 'None'\n",
    "\n",
    "            yield item\n",
    "            self.base.add_row('ville',name=item['name'],cp=item['cp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e6603",
   "metadata": {},
   "source": [
    "Un fois la liste des villes complétée, on la parcoure pour avoir la météo du jour dans chacune des villes. Sur le site `meteo-villes.com` avec la ville et le code postal dans l'url, on accède à des données sur la température, le vent et les précipitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b70b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Request\n",
    "from parapy.items import MeteoItem, VilleItem,DataBase \n",
    "import time\n",
    "import unidecode\n",
    "from sqlalchemy import create_engine, select\n",
    "import sqlalchemy as db\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class VilleSpider(scrapy.Spider):\n",
    "    name = 'city'\n",
    "    allowed_domains = ['www.linternaute.com']\n",
    "    start_urls = ['https://www.linternaute.com/ville/ile-de-france/region-11/villes']\n",
    "\n",
    "    try:\n",
    "      base = DataBase('region')\n",
    "      base.create_table('ville', name=db.String, cp=db.String)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "          yield Request(url=url, callback=self.parse_ville)\n",
    "            \n",
    "    def parse_ville(self, response):\n",
    "        liste_villes = response.css('.list--bullet')[1].css('li')\n",
    "\n",
    "        for ville in liste_villes:\n",
    "            item = VilleItem() \n",
    "            \n",
    "            #nom de la ville\n",
    "            try: \n",
    "              item['name'] = unidecode.unidecode(\"-\".join(ville.css('a::text').extract()[0].split(' ')[:-1]).lower())\n",
    "            except:\n",
    "              item['name'] = 'None'\n",
    "            \n",
    "            #cp de la ville\n",
    "            try: \n",
    "              item['cp'] = ville.css('a::text').extract()[0].split(' ')[-1].replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            except:\n",
    "              item['cp'] = 'None'\n",
    "\n",
    "            yield item\n",
    "            self.base.add_row('ville',name=item['name'],cp=item['cp'])\n",
    "\n",
    "\n",
    "\n",
    "class MeteoSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'meteo'\n",
    "    allowed_domains = ['www.meteo-villes.com']\n",
    "    \n",
    "    engine = create_engine(\"sqlite:///region.db\")\n",
    "    all_ville = DataBase('region').read_table('ville')\n",
    "    tbl = select([all_ville.columns.cp, all_ville.columns.name])\n",
    "    connection = engine.connect()\n",
    "    results = connection.execute(tbl).fetchall()\n",
    "\n",
    "    start_urls = [f'https://www.meteo-villes.com/previsions-meteo-{n[1]}-{n[0]}' for n in results]\n",
    "    try:\n",
    "      base = DataBase('meteo')\n",
    "      base.create_table('meteo_paris', low_temp=db.String, hight_temp=db.String, vent=db.String, pluie=db.String, date=db.String)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "          yield Request(url=url, callback=self.parse_meteo)\n",
    "            \n",
    "    def parse_meteo(self, response):\n",
    "        meteo = response.css('.city-summary-line')[0]\n",
    "\n",
    "        item = MeteoItem() \n",
    "            \n",
    "        #temperature min\n",
    "        try: \n",
    "            item['low_temp'] = meteo.css(\".city-summary-line__temp div::text\")[0].extract()\n",
    "        except:\n",
    "            item['low_temp'] = 'None'\n",
    "        \n",
    "        #temperature max\n",
    "        try: \n",
    "            item['hight_temp'] = meteo.css(\".city-summary-line__temp div::text\")[1].extract()\n",
    "        except:\n",
    "            item['hight_temp'] = 'None'\n",
    "        \n",
    "        #vent\n",
    "        try: \n",
    "            item['vent'] = meteo.css(\".city-summary-line__wind--speed span::text\").extract()[0]\n",
    "        except:\n",
    "            item['vent'] = 'None'\n",
    "        \n",
    "        #pluie\n",
    "        try: \n",
    "            item['pluie'] = meteo.css(\".city-summary-line__rain::text\").extract()[0].strip()\n",
    "        except:\n",
    "            item['pluie'] = 'None'\n",
    "        \n",
    "        #date\n",
    "        try: \n",
    "            item['date'] = str(time.ctime())\n",
    "        except:\n",
    "            item['date'] = 'None'\n",
    "        \n",
    "        #ville\n",
    "        try: \n",
    "            item['date'] = str(time.ctime())\n",
    "        except:\n",
    "            item['date'] = 'None'\n",
    "\n",
    "        yield item\n",
    "        self.base.add_row('meteo_paris', low_temp=item['low_temp'], hight_temp=item['hight_temp'], vent=item['vent'], pluie=item['pluie'], date=item['date'])\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(VilleSpider)\n",
    "process.crawl(MeteoSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
